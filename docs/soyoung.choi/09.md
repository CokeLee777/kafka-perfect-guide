# 09 멀티 노드 카프카 클러스터

## 1. 멀티 노드 카프카 클러스터 개요

- 분산 시스템으로서 카프카의 성능과 가용성을 함께 향상 시킬 수 있도록 구성
- 스케일 아웃 기반으로 노드 증설을 통해 카프카의 메시지 전송과 읽기 성능을 거의 선형적으로 증가 시킬 수 있음.
- 데이터 복제을 통해 분산 시스템 기반에서 카프카의 최적 가용성을 보장


## 2. 분산 시스템 구성을 위한 중요 요소
- 분산 시스템 도입을 위해서는 성능, 안정성, 가용성 측면에서 상세한 기능 검토가 요구
- 분산 시스템은 대량 데이터를 여러 노드간 분산 처리를 통해 빠르게 처리할 수 있는 큰 성능적 이점을 가지지만, 안정성과 가용성 측면에서 상대적인 단점을 가짐.

## 3. Scale Up/Out 방식

### 1. 단일 노드 구성 Scale Up 방식

처리하려는 데이터가 기하 급수적으로 늘어났을 때, 단방향 복제. 단일 노드 Scale Up.

- 단일 노드에 대해서만, 관리, 가용성 구성을 강화하면 되므로, 매우 안정적인 시스템 구성이 가능
- 소프트웨어에서 다양한 성능향상 기법을 도입하기 매우 시움
- 그러나, H/W 을 CPU Core, Memory 용량, 디스크 용량, 네트워크 Bandwidth 를 Scale Up 방식으로 증설하기에는 한계가 있다. (비용, H/W)

### 2. 다수의 노드로 분산 구성
- 개별 H/W 노드를 Scale Out 방식으로 증설하여, 대용량 데이터 성능 처리를 선형적으로 향상
- 다수의 H/W가 1/N 의 데이터를 처리. 각 노드가 각자의 처리를 하는 것.
  - 이 중 한 개의 노드에서만 장애가 발생해도, 올바른 데이터 처리가 되지 않음.
- 다수의 H/W로 구성하였으므로 빈번한 장애 가능성, 관리의 부담.
- 소프트웨어 자체에서 성능/가용성 처리 제약

### 3. 멀티 노드 카프카 클러스터
- 분산 시스템으로서 카프카의 성능과 가용성을 함께 향상 시킬 수 있도록 구성.
- 스케일 아웃 기반으로 노드 증설을 통해 카프카의 메시지 전송과 읽기 성능을 선형적으로 증가 시킬 수 있음.
- 데이터 복제을 통해 분산 시스템 기반에서 카프카의 최적 가용성을 보장.
![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/511cf15e-606b-4890-896c-ecf875f77620)

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/c23622a8-9cc6-4f7d-accd-3a6e7974c93b)


## 3. 카프카 리플리케이션
- 카프카는 개별 노드의 장애를 대비하여 높은 가용성을 제공
- 카프카 가용성의 핵심은 리클리케이션 복제이다.
- relplication 은 토픽 생성 시,  relplication factor 설정값을 통해 구성
- relplication factor 가 3이면, 원본 파티션과 복제 파티션을 포함하여 모두 3개의 파티션을 가짐을 의미
- relplication factor 의 개수는 브로커의 개수보다 클 수 없음.
- replication 의 동작은 토픽 내의 **개별 파티션!들을** 대상으로 적용
- Leader 는 파티션 단위이다.
- relplication factor 의 대상인 파티션들은 1개의 Leader 와 N개의 Follower 로 구성

## 4. 카프카 relplication 의 Leader와 Folloewer
- producer 와 consumer 는 Leader 파티션을 통해서 쓰기와 읽기 수행
- 파티션의 relplication 은 Leader 에서 Follwer 으로만 이뤄짐
- 실제로는 follower 가 Leader 파티션에 가서 읽어와 자기한테 write하는 방식으로 되어 있다.
- 파티션 리더를 관리하는 브로커는 producer/consumer 의 읽기/쓰기를 관리함과 동시에 **파티션 팔로우를 관리하는 브로커의 relplication 도 관리** (ISR - 잘 복제하고 있는지 관리. 잘 안되면 해당 복제본 제외 시킴)

- producer 는 리더 파티션이 있는 브로커에만  메시지를 쓰지만,
- Kafka 2.4 부터 consumer 는 follwer 로 부터 읽어들인다. => follower fetching

- acks = all : 복제본 까지 메시지가 다 전달 되어야, ack 를 주는 설정

## 4. 멀티 파티션의 relplication

- 파티션의 relplication 은 leader 에서 follwer 으로만 이뤄짐 (follwer 에서 leader 의 파티션을 읽어감)
- 파티션 리더를 관리하는 브로커는 producer / consumer 의 읽기 쓰기를 관리함과 동시에 파티션 팔로우를 관리하는 브로커의 relplication 도 관리

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/edb6f9a8-b5e9-4eb8-a609-efbcad6d3a26)


**👍ProducerConfig.BOOTSTRAP_SERVERS_CONFIG**

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/04942fd6-c2b4-4bc2-bc2f-2de8550d4bb4)


- producer 는 bootstrap.servers 에 기술되어 있는 브로커들의 List 를 기반으로 접속
- bootstrap.servers 는 브로커 Listener 들의 List
- **메타 정보를 가져오기 위한 리스트**
- 개별 Broker 들의 메타 정보를 서로 공유하고, producer 는 초기 접속 시 이 메타 정보를 가져와서 접속하려는 토픽의 파티션이 있는 브로커로 다시 접속

=> 사실 한 주소만 보내도, 동작할 수 있다.

브로커들이 공유하는 메타정보는 개별 브로커과 모두 동일하게 가지고 있다。

그리고 이 정보를 가져오기 위한 주소이다.

사실 메타정보를 위해선 하나의 주소만 알면 된다.

그러나, 만약 여러 브로커 주소를 입력하고, 한 브로커가 죽었다고 생각하면, 다른 브로커 주소를 통해 메타정보를 찾게 될 것 이다.

## 5. Zookeeper 개요

분산 시스템 간의 정보를 신속하게 공유하기 위하 코디네이션 시스템

여러 개의 개별 노드들의 상태 정보 등을 마스터가 관리했는데,

이에 문제가 많았음. 그래서 이를 zookeeper 라는 시스템이 대신해줌.

### 1. Zookeeper 의 이해

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/5b724bac-f8a7-42ac-8a9d-f7ceb54945b8)

- 클러스터 내 개별 노드의 **중요한 상태 정보를 관리** (Z Node 가 관리)하며
- 분산 시스템에서 리더 노드 (Controller)를 선출하는 역할 등을 수행
- 개별 노드 간 상태 정보의 동기화를 위한 복잡한 Lock 관리 기능 제공
- 간편한 디렉토리 구조 기반의 Z node 를 활용
- Z node 는 개별 노드(카프카 노드) 의 중요 정보를 담고 있음.
- 개별 노드 (카프카 노드) 들은 Zookeeper 의 Z node 를 계속 모니터링하며, Z node에서 변경 발생 시 watch event 가 트리거 되어 변경 정보가 개별 노드들에 통보
- zookeeper 자체의 클러스터링 기능 제공

### 2. 카프카 클러스터에서 Zookeeper 의 역할

1. controller broker 선출 (election)
- controller 는 여러 브로커들에서 파티션 Leader 선출을 수행
- 먼저 접속한 애가 controller 

2. Kafka 클러스터 내 Broker 의 Membership 관리
- 클러스터 
- 브로커가 새롭게 들어왔는지, 떠났는지 관리
  - 이를 controller 에게 보고. (ex. 몇 번 브로커가 heartbeat 안 온다고 죽었다고 보고 받음.)
  - controller 는 zookeeper 로부터 broker 추가/Down 등의 정보를 받으면 해당 broker 로 인해 영향을 받는 파티션들에 대해서 새로운 Leader Election 을 수행

3. Topic 정보 관리
- 토픽의 파티션 replicas 등의 정보를 가짐

### 3. Zookeeper에서 kafka cluster 정보 관리

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/9545e1af-3505-400a-8023-0b6dbe2a3358)


- 모든 카프카 브러카 브로커는 주기적으로 zookeeper 에 접속하면서 session haearbeat 을 전송하여 자신의 상태를 보고함.
- zookeeper 는 zookeeper.session.timeout.ms 이내에 heartbeat 을 받지 못하면 해당 브로커의 노드 정보를 삭제하고, controller 노드에게 변경 사실을 통보
- controller 노드는 다운된 브로커가 관리하는 파티션들에 대해서 새로운 파티션 leader election 을 수행
- 만일 다운된 브로커가 controller 이면 모든 노드에게 해당 사실을 통보하고 가장 먼저 접속한 다른 브로커가 controller 가 됨.

### 4. controller 의 leader election 수행 프로세스

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/6d10a431-22d4-450d-a59d-c077d97daa57)

1. Broker # 3 이 Shutdown 되고 zookeeper 는 session 기간 동안 HeartBeat 이 오지 않으므로 해당 브로커 노드 정보 갱신

2. controller 는 zookeeper 를 모니터링 하던 중 watch Event 로 Broker#3 에 대한 down 정보를 받음.

3. controller 는 다운된 브로커가 관리하던 파티션들에 대해 새로운 Leader/Follower 결정

4. 결정된 새로운 Leader /Follower 정보를 Zookeeper 에 저장하고 해당 파티션을 복제하는 모든 브로커에게 새로운 Leader/Follower 정보를 전달하고 새로운 Leader 로부터 복제 수행할 것을 요청

5. controller 는 모든 브로커가 가지는 metadatacache 를 새로운 Leader/Follower 정보로 갱신할 것을 요청.


## 6. ISR
- Leader 브로커가 관리
- follower 들은 누구라도 Leader 가 될 수 있지만, 단 **ISR** 내에 있는 Follower 들만 가능. 
- ISR 은 건강한 Follower 를 관리하겠다는 것!
  - 기준이 있음. 이에 속하지 못하면 lEADER 가 될 수 없음.
- 파티션의 Ledader 브로커는 Follower 파티션의 브로커들이 Leader가 될 수 있는지, isr 에 속할 수 있는지, 지속적으로 모니터링 수행하여 isr 을 관리
- leader 파티션의 메시지를 follower 가 빠르게 복제하지 못하고 뒤쳐질 경우 isr에서 해당 follower 는 제거되며, leader 가 문제가 생길 때, 차기 leader 가 될 수 없음.

### 1. ISR 조건

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/57e16afb-e262-416b-8266-69a321706637)


1. 브로커가 Zookeeper 에 연결되어 있어야함. zookeeper.sesstion.timeout.ms 로 지정된 기간 (기본 6초, 최대 18초 )내에 Heartbeat 을 지속적으로 zookeeper 로 보냄.
2. replica.lag.time.max.ms 로 지정된 기간(기본 10초, 최대 30초)내에 Leader 의 메시지를 지속적으로 가져 가야 함.

- Leader 파티션이 있는 브로커는 follower 들이 제대로 데이터를 가져가는지 모니터링하면서 ISR 관리
- 실제로 follower 가 leader 에게 데이터를 보고 => fetcher 로 

**❓어떻게 시간을 체크**
- follower 는 leader 에게 fetch 요청을 수행. fetch 요청에는 follower 가 다음에 읽을 메시지의 offset 번호를 포함.
- leader 는 follower 가 요청한 offset 번호와 현재 leader partition 의 가장 최신 offset 번호를 비교하여 follower 가 얼마나 leader 데이터 복제를 잘 수행하고 있는지 판단


### 2. ISR 의 적용

![image](https://github.com/CokeLee777/kafka-perfect-guide/assets/66711073/dcfec3fe-cb08-4a6d-acc7-67fc6197dfb8)

### 3. min.insync.replicas 의 이해

min.insync.replicas 파라미터는, 브로커의 설정값(or Topic 설정값)으로, producer 가 acks=all 로 성공적으로 메시지를 보낼 수 있는 최소한의 ISR 브로커 개수를 의미

### 4. Producer 의 acks 설정에 따른 send 방식 - acks all

![alt text](<images/9-10 acis 설정에 따른 send 방식.png>)

- producer 는 leader broker 가 메시지 a 를 정상적으로 받은 뒤 모든 replicator 에 복제를 수행한 뒤에 보내는 ack 메시지를 받은 후 다음 메세지인 메시지 b를 바로 전송. 만약 오류 메시지를 브로커로부터 받으면 메시지 a를 재전송
- 메시지 a 가 모든 replicator 에 완벽하게 복사되었는지의 여부까지 확인 후에 메시지 b를 전송
- 메시지 손실이 되지 않도록 모든 장애 상황을 감안한 전송 모드이지만 ack 를 오래 기다려야 하므로 상대적으로 전송속도가 느림

- replication-factor = 3, min.insync.replicas=2 일 때, 하나 죽어도 acks 를 잘 보냄. 두 개 죽으면 error:NOT_ENOUGH_REPLICAS 발생

### 5. Preferred Leader Election
![alt text](<images/9-11 Preferred Leader Election.png>)


파티션 별로 최초 할당된 Leader/Follower Broker 설정을 Preferred Broker 로 그대로 유지

Broker 가 Shutdown 후 재 기동될 때 Preferred Leader Broker 를 일정 시간 이후에 재선출

auto.leader.rebalance.enable=true 로 설정하고, leader.imbalance.check.interval.seconds 를 일정 시간을 설정 (기본 300초)

![alt text](<images/9-12 Preferred Leader Election 2.png>)

Broker #2, #3 가 모두 shutdown 되면 partition #1, #2, #3 의 Leader Broker # 1이 됨.

Broker #1에 메시지가 추가로 계속 유입된 후 Broker #1 까지 shutdown 될 경우 이후 Broker#2,#3 이 재기동되어도 Partition #1, #2, #3 의 Leader Broker 가 될 수 없음.

### 6. Unclean Leader Election

기존의 Leader 브로커가 오랜 기간 살아나지 않을 경우, 

복제가 완료되지 않은 (Out of sync) Follower Broker 가 Leader 가 될지 결정해야 함.

이때 기존의 Leader 브로커가 가진 메시지 손실 여부를 감수하고

복제가 완료되지 않은 follower broker 가 leader 가 되려면,

unclean.leader.election.enable=true 로 설정하고 unclean leader election 수행


